---

output: html_document
---
This report was created the using the behavseqanalyser `r version`, behaviour were grouped  using the `r groupingby` categorisation (date: `r Sys.Date()`).


#This is an analysis of the `r Name_project` project, done with the behavseqanalyser `r version`


The data is grouped by `r Projects_metadata$group_by `.
Data transformation: `r calcul_text`.

```{r, evaluate =FALSE, echo = FALSE}

summary (as.factor(metadata$groupingvar))

```



---


We grouped the variables following the `r groupingby` argument to get `r length(names(behav_gp))-3` behavior categories. We used the folowing time windows and got `r length(names(behav_gp))-4` x `r nrow (Timewindows)` **=
`r (length(names(behav_gp))-4)* nrow (Timewindows)` variables** :

```{r, results='asis', echo=FALSE}
pander::pandoc.table(Timewindows)
```
Note that the last window might be truncated if not all dataset is achieving 900 min after light on.

We then run a random forest to get the variables in order of importance to distinguish the groups. We then take the best 20 and run the random forest again (such that the Gini scores obtained will not depend on the initial number of variables). We plot here the table of variables ordered by weight:
```{r, echo=FALSE}

varImpPlot(HCS.rf)
```



Let's take a teshold of importance (Gini > `r import_treshold`) and get all variables satisfying the filter, or at least 8 variables:

```{r, echo=FALSE}

pander::pander(Variables_list)


```

#Plotting
First, lets plot the 2 most discriminative variables following the random forest:

```{r}


 Plot = Multi_datainput_m [,names(Multi_datainput_m) %in% as.character(R2 [1:2,1]) ]
  Plot = cbind(Multi_datainput_m$groupingvar, Plot)
  Title_plot = paste0(names (Plot) [2],"x",names (Plot) [3])
  names (Plot) = c("groupingvar","disciminant1", "discriminant2")
  p=ggplot (Plot, aes (y= disciminant1, x=discriminant2, color= groupingvar))+
    geom_point()+
    labs(title = Title_plot)+
    #scale_x_log10() + scale_y_log10()+
    scale_colour_grey() + theme_bw()+
      theme(legend.position='none')
print(p)  
```


Here, we plot the first two or threecomponents obtained after a ICAperformed on the reduced data:

```{r, echo=FALSE}
print(pls)  
pls2
```


#PCA strategy


```{r, results='asis', echo=FALSE}
if (PCA_pval$p.value< .05) {pander::pandoc.strong("The PCA strategy shows that the behavior profile of the two groups of animal are not identical.")
}else {pander::pandoc.strong("The PCA strategy could not tell the two groups apart.")
  
}


```

We performed a PCA on the data and tested whether the groups show a difference in their first component score using a Mann-Whitney or a Kruskal-Wallis rank sum test (if more than 2 groups exists).
We plot here the first component in a boxplot:

```{r, echo=FALSE}

print(boxplotPCA1)
```

NB: This strategy is pretty good against type I errors. On the other hand, it may well oversee existing differences.

#SVM

We perform a SVM on the total data or the reduced data and compare the results. For that with split the data in training and test sets, tune the svm for best parameters and then run the svm and gives the overall accuracy (kappa) as the output. This accuracy (`r Accuracyreal`) was tested for significance, using a permutation strategy. We performed `r length(Acc_sampled)` permutations.
(What it does is permute the elements in random groups in the training data, tune a svm and apply it to the (non-randomised) test set, its prediction (kappa score) is saved.
We use a Binomial confidence interval to calculate a p value. )



```{r, echo=FALSE}
if (nrow(metadata) < 22) {print("there is not enough data to try to do a svm")
  knitr::knit_exit()
}
```

```{r, results='asis', echo=FALSE}
if (R[3]< 0.05) {pander::pandoc.strong("The SVM procedure suggest that the behavior profile of the two groups of animal are not identical.")
}else {pander::pandoc.strong("The SVM procedure could not tell the two groups apart.")

}


```

--- 

Details:
```{r, results='asis', echo=FALSE }
  print(Accuracy)

```

distribution of the accuracy scores with permuted labels, with adding a vertical line at the Score obtained using the real groups.

```{r, echo=FALSE}

hist(Acc_sampled, breaks=c(0:15)/15*2-1)
abline(v = Accuracyreal, col="Red")

```
P value calculation:
```{r}
                                 # Exports `binconf`
k <- sum(abs(Acc_sampled) >= abs(Accuracyreal))   # Two-tailed test
R=binconf(k, length(Acc_sampled), method='exact')
print(zapsmall(R)) # 95% CI by default
 save.image(file= "results.rdata") 
  
```




---
metadata used for the analysis

```{r, echo=FALSE}
DT::datatable(t(Projects_metadata),
              options = list(
  pageLength =100,
  lengthMenu = c( 5, 100)
),caption = "master metadata")

```

---

```{r, echo=FALSE}
metadata$`real time start`= as.character(metadata$`real time start`)
DT::datatable (metadata,caption = "full metadata",
               extensions = 'Buttons', options = list(
    dom = 'Bfrtip',
    buttons = 
       list(
        extend = 'collection',
        buttons = c('csv', 'excel'),
        text = 'Download'
      )))


```






